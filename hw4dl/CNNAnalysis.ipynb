{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d22b1621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/shobhita/miniconda3/envs/lpips2/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/data/vision/phillipi/perception/hw4dl_final_project/hw4dl/loaders\")\n",
    "sys.path.append(\"/data/vision/phillipi/perception/hw4dl/datasets\")\n",
    "from loaders.map2loc_loader import Map2Loc\n",
    "from models.shared_cnn import VariableCNNBackbone\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.distributions.normal import Normal\n",
    "from torchvision import transforms\n",
    "import pdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "62a065ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2213d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"pixel\"\n",
    "train_dataset = Map2Loc(root_dir=f'datasets/map2loc_{task}', csv_file='description.csv')\n",
    "test_dataset = Map2Loc(root_dir=f'datasets/map2loc_{task}_test', csv_file='description.csv')\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [int(len(train_dataset) * 0.8), int(len(train_dataset) * 0.2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1bb195b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ROOT_DIR = \"/data/vision/phillipi/perception/hw4dl_final_project\"\n",
    "dir = os.path.join(ROOT_DIR, 'hw4dl/datasets/map2loc_patch/')\n",
    "df = pd.read_csv(os.path.join(dir, 'description.csv'))\n",
    "x = df['X'].to_numpy()\n",
    "y = df['Y'].to_numpy()\n",
    "toplot_x = []\n",
    "toplot_y = []\n",
    "patch_size = 2\n",
    "for i, j in zip(x, y):\n",
    "    x_grid, y_grid = np.meshgrid(list(range(i, i + patch_size + 1)), list(range(j, j + patch_size + 1)))\n",
    "    toplot_x += list(x_grid.flatten())\n",
    "    toplot_y += list(y_grid.flatten())\n",
    "plt.figure()\n",
    "plt.plot(toplot_x, toplot_y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "7e11b746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 6,\n",
       " ...]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toplot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fa5d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "ROOT_DIR = \"/data/vision/phillipi/perception/hw4dl_final_project\"\n",
    "dir = os.path.join(ROOT_DIR, 'hw4dl/datasets/map2loc_pixel/')\n",
    "df = pd.read_csv(os.path.join(dir, 'description.csv'))\n",
    "x = df['X'].to_numpy()\n",
    "y = df['Y'].to_numpy()\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8cabc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f766e3f1000>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(transforms.ToPILImage()(train_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "97dfc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7d6b25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if task == \"pixel\":\n",
    "    layer_shapes = (1, 16, -1, 32, -1, 64, 128, 256, 'fc512', 'fc2')\n",
    "else:\n",
    "    layer_shapes = (1, 16, -1, 32, -1, 64, 128, 256, 'fc512', 'fc18')\n",
    "split_idx = 7\n",
    "num_heads = 3\n",
    "n_epochs = 5\n",
    "model = VariableCNNBackbone(layer_shapes, split_idx, num_heads, input_size=(15,15), task=task)\n",
    "\n",
    "def make_sigma_positive(sigma):\n",
    "  return torch.log(1 + torch.exp(sigma)) + 1e-6\n",
    "\n",
    "def nll_loss(outputs, labels):\n",
    "  \"\"\"\n",
    "  Negative log likelihood loss\n",
    "  Outputs: B x 2\n",
    "  Labels: B x 1\n",
    "  \"\"\"\n",
    "  mu, sigma = torch.split(outputs, 1, dim=1)\n",
    "  sigma = make_sigma_positive(sigma)\n",
    "  cond_dist_x = Normal(loc=mu, scale=sigma)\n",
    "  loss = -cond_dist_x.log_prob(labels)\n",
    "\n",
    "  return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c4597c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:10<00:00, 12.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial val loss,  6.916227909088135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:43<00:00, 11.59it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:10<00:00, 12.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 4.482385175228119, val loss: 4.337122785568237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:44<00:00, 11.19it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:10<00:00, 12.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 4.37879837179184, val loss: 4.335493774414062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:45<00:00, 10.99it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:11<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train loss: 4.37698360157013, val loss: 4.3347251605987545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:45<00:00, 11.02it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:10<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train loss: 4.375940751552582, val loss: 4.3343035583496095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:44<00:00, 11.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 125/125 [00:10<00:00, 12.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train loss: 4.375279059886933, val loss: 4.334030071258545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:25<00:00, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.4790347399422155\n",
      "Done :)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def eval(model, model_type, scramble_batches, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for inputs, labels in tqdm(loader):\n",
    "        batch_loss = 0.0\n",
    "        inputs, labels = inputs.type(torch.float32).to(device), labels.type(torch.float32).to(device)\n",
    "        outputs = model(inputs)\n",
    "        if model_type == \"shared\":\n",
    "            for head_i, output in enumerate(outputs):\n",
    "                if scramble_batches:\n",
    "                  batch_loss += criterion(output, labels[:,:,head_i])\n",
    "                else:\n",
    "                  batch_loss += criterion(output, labels)\n",
    "        else:\n",
    "            batch_loss = criterion(outputs, labels)\n",
    "        total_loss += batch_loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nll_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "val_loss = eval(model, \"shared\", False, val_loader, criterion, device)\n",
    "print(\"Initial val loss, \", val_loss)\n",
    "\n",
    "# Training loop\n",
    "for i in range(n_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs = inputs.type(torch.float32)\n",
    "        labels = labels.type(torch.float32)\n",
    "        batch_loss = 0.0\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        for head_i, output in enumerate(outputs):\n",
    "            batch_loss += criterion(output, labels)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += batch_loss.item()\n",
    "\n",
    "    train_loss = total_train_loss / len(train_loader)\n",
    "    val_loss = eval(model, \"shared\", False, val_loader, criterion, device)\n",
    "    print(f\"Epoch {i}, train loss: {train_loss}, val loss: {val_loss}\")\n",
    "\n",
    "test_loss = eval(model, \"shared\", False, test_loader, criterion, device)\n",
    "print(f\"Test loss: {test_loss}\")\n",
    "\n",
    "# print(f\"Saving model and config\")\n",
    "# save_model(model, dict(test_loss=test_loss), args)\n",
    "\n",
    "print(\"Done :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cf25e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE: 0.17630185567380602\n",
      "Mean Sigma: 0.18057789777345482\n",
      "Percentage of samples classified correctly: 0.2\n"
     ]
    }
   ],
   "source": [
    "def get_mask_from_gaps(gaps, shape):\n",
    "  mask = np.ones(shape)\n",
    "  for gap in gaps:\n",
    "    xx, yy = np.meshgrid(list(range(gap[0], gap[1] + 1)), list(range(gap[2], gap[3] + 1)))\n",
    "    mask[xx, yy] = 0\n",
    "  return mask.flatten()\n",
    "def score_cnn_performance(model,\n",
    "                             test_loader,\n",
    "                             epi_threshold:float=0.1,\n",
    "                             device:str=\"cpu\",\n",
    "                             )->plt.Axes:\n",
    "  model.eval()\n",
    "  testx, testy = np.meshgrid(np.arange(15), np.arange(15))\n",
    "  coords = []\n",
    "  all_inputs = []\n",
    "  for x, y in zip(testx.ravel(), testy.ravel()):\n",
    "    inputs = np.zeros((15, 15))\n",
    "    inputs[x, y] = 1\n",
    "    all_inputs.append(torch.tensor(inputs))\n",
    "    coords.append((x, y))\n",
    "  coords = np.array(coords)\n",
    "  inputs = torch.stack(all_inputs).unsqueeze(1).type(torch.float32).to(device)\n",
    "  outputs = model(inputs)\n",
    "  values = torch.stack(outputs).squeeze(-1)\n",
    "  means = values[:, :, 0].mean(dim=0)\n",
    "  sigma = torch.sqrt(\n",
    "    torch.mean(make_sigma_positive(values[:, :, 1]) + torch.square(values[:, :, 0]), dim=0) - torch.square(means))\n",
    "  epistemic_sigma = torch.std(values[:, :, 0], dim=0)\n",
    "  epistemic_sigma = epistemic_sigma.detach().cpu().numpy()\n",
    "  classified_data_region = epistemic_sigma < epi_threshold\n",
    "\n",
    "\n",
    "  mask = get_mask_from_gaps(test_loader.gaps, test_loader.shape).astype(bool)\n",
    "\n",
    "  x_input = (2 * testx.ravel()) / test_loader.shape[0] - 1\n",
    "  y_input = (2 * testy.ravel()) / test_loader.shape[0] - 1\n",
    "  gt_mean = test_loader.polyf(x_input, y_input)\n",
    "  gt_var = test_loader.varf(x_input, y_input)\n",
    "\n",
    "  samples_correct = np.sum(\n",
    "    np.logical_and(classified_data_region, mask)\n",
    "  ) + np.sum(np.logical_and(np.logical_not(classified_data_region), np.logical_not(mask)))\n",
    "  total_samples = len(testx) * len(testy)\n",
    "\n",
    "  means_arr = means.detach().cpu().numpy()\n",
    "  sigma_arr = sigma.detach().cpu().numpy()\n",
    "  mean_mse = np.mean(np.square(means_arr[mask] - gt_mean[mask]))\n",
    "  mean_sigma = np.mean(np.square(np.square(sigma_arr[mask]) - gt_var[mask]))\n",
    "  print(f\"Mean MSE: {mean_mse}\")\n",
    "  print(f\"Mean Sigma: {mean_sigma}\")\n",
    "  print(f\"Percentage of samples classified correctly: {samples_correct / total_samples}\")\n",
    "  return mean_mse, mean_sigma, samples_correct/total_samples\n",
    "\n",
    "mean_mse, mean_sigma, classified = score_cnn_performance(model, test_dataset, epi_threshold=0.001,device=device)\n",
    "all_results[split_idx] = {\n",
    "    'mean_mse': mean_mse,\n",
    "    'mean_sigma': mean_sigma,\n",
    "    'correct': classified\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b93efd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 648x432 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "testx, testy = np.meshgrid(np.arange(15), np.arange(15))\n",
    "x_input = (2 * testx.ravel()) / test_dataset.shape[0] - 1\n",
    "y_input = (2 * testy.ravel()) / test_dataset.shape[0] - 1\n",
    "gt_mean = test_dataset.polyf(x_input, y_input)\n",
    "gt_var = test_dataset.varf(x_input, y_input)\n",
    "\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(5,5))\n",
    "# fig.tight_layout()\n",
    "plt.figure(figsize=(9,6))\n",
    "ax1 = plt.subplot(231)\n",
    "ax2 = plt.subplot(233)\n",
    "ax3 = plt.subplot(234)\n",
    "ax4 = plt.subplot(235)\n",
    "ax5 = plt.subplot(236)\n",
    "# plt.tight_layout()\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "axes = [ax1, ax2, ax3, ax4, ax5]\n",
    "# axes = axes.flatten()\n",
    "axes[0].imshow(gt_mean.reshape((15,15)))\n",
    "axes[0].set_title(\"True function\")\n",
    "axes[1].imshow(gt_var.reshape((15,15)))\n",
    "axes[1].set_title(\"True variance\")\n",
    "\n",
    "all_inputs = []\n",
    "for x, y in zip(testx.ravel(), testy.ravel()):\n",
    "    inputs = np.zeros((15, 15))\n",
    "    inputs[x, y] = 1\n",
    "    all_inputs.append(torch.tensor(inputs))\n",
    "inputs = torch.stack(all_inputs).unsqueeze(1).type(torch.float32).to(device)\n",
    "outputs = model(inputs)\n",
    "values = torch.stack(outputs).squeeze(-1)\n",
    "means = values[:, :, 0].mean(dim=0)\n",
    "means_arr = means.detach().cpu().numpy()\n",
    "axes[2].imshow(means_arr.reshape((15,15)))\n",
    "axes[2].set_title(\"Predicted function\")\n",
    "\n",
    "sigma = torch.sqrt(\n",
    "torch.mean(make_sigma_positive(values[:, :, 1]) + torch.square(values[:, :, 0]), dim=0) - torch.square(means))\n",
    "sigma = sigma.detach().cpu().numpy()\n",
    "epistemic_sigma = torch.std(values[:, :, 1], dim=0)\n",
    "epistemic_sigma = epistemic_sigma.detach().cpu().numpy()\n",
    "\n",
    "axes[3].imshow(np.square(sigma).reshape((15,15)))\n",
    "axes[3].set_title(\"Predicted variance\")\n",
    "\n",
    "axes[4].imshow(np.square(epistemic_sigma).reshape((15,15)))\n",
    "axes[4].set_title(\"Epistemic variance\")\n",
    "for gap in test_dataset.gaps:\n",
    "    for i in range(4):\n",
    "        width = gap[1] - gap[0]\n",
    "        height = gap[3] - gap[2]\n",
    "        x1, y1 = gap[0], gap[2]\n",
    "        rect = Rectangle((x1, y1), width, height, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        axes[i].add_patch(rect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "da979e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4</th>\n",
       "      <th>1</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_mse</th>\n",
       "      <td>0.177693</td>\n",
       "      <td>0.175486</td>\n",
       "      <td>0.176302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_sigma</th>\n",
       "      <td>0.181809</td>\n",
       "      <td>0.180653</td>\n",
       "      <td>0.180578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <td>0.337778</td>\n",
       "      <td>0.213333</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   4         1         7\n",
       "mean_mse    0.177693  0.175486  0.176302\n",
       "mean_sigma  0.181809  0.180653  0.180578\n",
       "correct     0.337778  0.213333  0.200000"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fbfee9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.savefig(*args, **kwargs)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbfd1e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 4, 2, 4), (7, 12, 7, 12)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader.gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b720ad62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_network_performance(model:nn.Module,\n",
    "#                              toy_loader:PolyData,\n",
    "#                              device:str=\"cpu\",\n",
    "#                              )->plt.Axes:\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim(toy_loader.lower, toy_loader.upper)\n",
    "samples = np.linspace(toy_loader.lower, toy_loader.upper, 1000)\n",
    "var = toy_loader.varf(samples)\n",
    "ax.scatter(toy_loader.x, toy_loader.y, marker='.', alpha=0.1, label=\"Training Data\")\n",
    "print(toy_loader.x)\n",
    "ax.plot(samples, toy_loader.polyf(samples), label=\"True Function\")\n",
    "ax.fill_between(samples, toy_loader.polyf(samples) - var, toy_loader.polyf(samples) + var, alpha=0.5, label=\"True Variance\")\n",
    "\n",
    "torch_input = torch.unsqueeze(torch.tensor(samples, dtype=torch.float32), 1).to(device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.scramble_batches = False\n",
    "outputs = model(torch_input)\n",
    "values = torch.stack(outputs).squeeze(-1)\n",
    "for i in range(values.shape[0]):\n",
    "ax.plot(samples, values[i,:,0].detach().cpu().numpy(), alpha=0.5, label=\"_Network Prediction\")\n",
    "means = values[:,:,0].mean(dim=0)\n",
    "sigma = torch.sqrt(torch.mean(make_sigma_positive(values[:,:,1]) + torch.square(values[:,:,0]), dim=0) - torch.square(means))\n",
    "epistemic_sigma = torch.std(values[:,:,0], dim=0)\n",
    "means = means.detach().cpu().numpy()\n",
    "sigma = sigma.detach().cpu().numpy()\n",
    "epistemic_sigma = epistemic_sigma.detach().cpu().numpy()\n",
    "print(epistemic_sigma)\n",
    "ax.plot(samples, means, label=\"Mean Prediction\")\n",
    "ax.fill_between(samples, means - np.square(sigma), means + np.square(sigma), alpha=0.5, label=\"Predicted Variance\")\n",
    "ax.fill_between(samples, means - np.square(epistemic_sigma), means + np.square(epistemic_sigma), alpha=0.5, label=\"Epistemic Variance\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "55333768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import torch.nn as nn\n",
    "# from hw4dl.loaders.toy_loader import PolyData\n",
    "import numpy as np \n",
    "# from hw4dl.tools.manage_models import load_model, get_most_recent_model\n",
    "# from hw4dl.train import make_polyf, make_sigma_positive\n",
    "from torch.utils.data import DataLoader\n",
    "# from hw4dl.loaders.toy_loader import construct_intervals\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "75ce5078",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (225,3,3) (225,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[195], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m gt_mean \u001b[38;5;241m=\u001b[39m polyf(x_input, y_input)\n\u001b[1;32m     27\u001b[0m gt_var \u001b[38;5;241m=\u001b[39m varf(x_input, y_input)\n\u001b[1;32m     29\u001b[0m samples_correct \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\n\u001b[0;32m---> 30\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_and\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassified_data_region\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m ) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlogical_and(np\u001b[38;5;241m.\u001b[39mlogical_not(classified_data_region), np\u001b[38;5;241m.\u001b[39mlogical_not(mask)))\n\u001b[1;32m     32\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(testx) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(testy)\n\u001b[1;32m     34\u001b[0m means_arr \u001b[38;5;241m=\u001b[39m means\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (225,3,3) (225,) "
     ]
    }
   ],
   "source": [
    "def get_mask_from_gaps(gaps, shape):\n",
    "    mask = np.ones(shape)\n",
    "    for gap in gaps:\n",
    "        xx, yy = np.meshgrid(list(range(gap[0], gap[1] + 1)), list(range(gap[2], gap[3] + 1)))\n",
    "        mask[xx, yy] = 0\n",
    "    return mask.flatten()\n",
    "model.eval()\n",
    "testx, testy = np.meshgrid(np.arange(15), np.arange(15))\n",
    "coords = []\n",
    "all_inputs = []\n",
    "for x, y in zip(testx.ravel(), testy.ravel()):\n",
    "    inputs = np.zeros((15, 15))\n",
    "    inputs[x, y] = 1\n",
    "    all_inputs.append(torch.tensor(inputs))\n",
    "    coords.append((x, y))\n",
    "coords = np.array(coords)\n",
    "inputs = torch.stack(all_inputs).unsqueeze(1).type(torch.float32).to(device)\n",
    "outputs = model(inputs)\n",
    "values = torch.stack(outputs).squeeze(-1)\n",
    "means = values[:,:,0].mean(dim=0)\n",
    "sigma = torch.sqrt(torch.mean(make_sigma_positive(values[:,:,1]) + torch.square(values[:,:,0]), dim=0) - torch.square(means))\n",
    "epistemic_sigma = torch.std(values[:,:,0], dim=0)\n",
    "epistemic_sigma = epistemic_sigma.detach().cpu().numpy()\n",
    "classified_data_region = epistemic_sigma < 0.005\n",
    "\n",
    "gaps = [(2, 4, 2, 4), (7, 10, 7, 10)]\n",
    "shape = (15, 15)\n",
    "mask = get_mask_from_gaps(gaps, shape).astype(bool)\n",
    "\n",
    "x_input = (2 * testx.ravel()) / shape[0] - 1\n",
    "y_input = (2 * testy.ravel()) / shape[0] - 1\n",
    "gt_mean = polyf(x_input, y_input)\n",
    "gt_var = varf(x_input, y_input)\n",
    "\n",
    "samples_correct = np.sum(\n",
    "np.logical_and(classified_data_region, mask)\n",
    ") + np.sum(np.logical_and(np.logical_not(classified_data_region), np.logical_not(mask)))\n",
    "total_samples = len(testx) * len(testy)\n",
    "\n",
    "means_arr = means.detach().cpu().numpy()\n",
    "sigma_arr = sigma.detach().cpu().numpy()\n",
    "mean_mse = np.mean(np.square(means_arr[mask] - gt_mean[mask]))\n",
    "mean_sigma = np.mean(np.square(np.square(sigma_arr[mask]) - gt_var[mask]))\n",
    "print(f\"Mean MSE: {mean_mse}\")\n",
    "print(f\"Mean Sigma: {mean_sigma}\")\n",
    "print(f\"Percentage of samples classified correctly: {samples_correct/total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0d44f4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0,  3,  6,  9, 12],\n",
       "        [ 0,  3,  6,  9, 12],\n",
       "        [ 0,  3,  6,  9, 12],\n",
       "        [ 0,  3,  6,  9, 12],\n",
       "        [ 0,  3,  6,  9, 12]]),\n",
       " array([[ 0,  0,  0,  0,  0],\n",
       "        [ 3,  3,  3,  3,  3],\n",
       "        [ 6,  6,  6,  6,  6],\n",
       "        [ 9,  9,  9,  9,  9],\n",
       "        [12, 12, 12, 12, 12]])]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "e8d7ea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE: 0.19968671483872327\n",
      "Mean Sigma: 0.03926807387848733\n",
      "Percentage of samples classified correctly: 0.5688888888888889\n"
     ]
    }
   ],
   "source": [
    "def get_mask_from_gaps(gaps, shape):\n",
    "    mask = np.ones(shape)\n",
    "    for gap in gaps:\n",
    "        xx, yy = np.meshgrid(list(range(gap[0], gap[1] + 1)), list(range(gap[2], gap[3] + 1)))\n",
    "        mask[xx, yy] = 0\n",
    "    return mask\n",
    "model.eval()\n",
    "testx, testy = np.meshgrid(np.arange(15, step=3), np.arange(15, step=3))\n",
    "testxx, testyy = np.meshgrid(np.arange(15), np.arange(15))\n",
    "coords = []\n",
    "all_inputs = []\n",
    "for x, y in zip(testx.ravel(), testy.ravel()):\n",
    "    inputs = np.zeros((15, 15))\n",
    "    inputs[x:x+3, y:y+3] = 1\n",
    "    inputs = torch.tensor(inputs)\n",
    "    all_inputs.append(inputs)\n",
    "    coords.append((x, y))\n",
    "inputs = torch.stack(all_inputs).unsqueeze(1).type(torch.float32).to(device)\n",
    "outputs = model(inputs)\n",
    "values = torch.stack(outputs).squeeze(-1)\n",
    "means = values[:,:,0].mean(dim=0)\n",
    "means_arr = means.detach().cpu().numpy()\n",
    "means_arr = np.hstack([np.vstack(means_arr[i:i+5,:,:]) for i in range(0, 25, 5)])\n",
    "sigma = torch.sqrt(torch.mean(make_sigma_positive(values[:,:,1]) + torch.square(values[:,:,0]), dim=0) - torch.square(means)).detach().cpu().numpy()\n",
    "sigma_arr = np.hstack([np.vstack(sigma[i:i+5,:,:]) for i in range(0, 25, 5)])\n",
    "epistemic_sigma = torch.std(values[:,:,0], dim=0).detach().cpu().numpy()\n",
    "epistemic_sigma = np.hstack([np.vstack(epistemic_sigma[i:i+5,:,:]) for i in range(0, 25, 5)])\n",
    "classified_data_region = epistemic_sigma < 0.001\n",
    "#     all_inputs.append(torch.tensor(inputs))\n",
    "#     coords.append((x, y))\n",
    "# coords = np.array(coords)\n",
    "# inputs = torch.stack(all_inputs).unsqueeze(1).type(torch.float32).to(device)\n",
    "# outputs = model(inputs)\n",
    "# values = torch.stack(outputs).squeeze(-1)\n",
    "# means = values[:,:,0].mean(dim=0)\n",
    "# sigma = torch.sqrt(torch.mean(make_sigma_positive(values[:,:,1]) + torch.square(values[:,:,0]), dim=0) - torch.square(means))\n",
    "# epistemic_sigma = torch.std(values[:,:,0], dim=0)\n",
    "# epistemic_sigma = epistemic_sigma.detach().cpu().numpy()\n",
    "# classified_data_region = epistemic_sigma < 0.0005\n",
    "\n",
    "gaps=[(0,5,0,5), (6,14,6,14)]\n",
    "shape = (15, 15)\n",
    "mask = get_mask_from_gaps(gaps, shape).astype(bool)\n",
    "\n",
    "x_input = (2 * testxx.ravel()) / shape[0] - 1\n",
    "y_input = (2 * testyy.ravel()) / shape[0] - 1\n",
    "gt_mean = polyf(x_input, y_input).reshape((15,15))\n",
    "gt_var = varf(x_input, y_input).reshape((15,15))\n",
    "\n",
    "samples_correct = np.sum(\n",
    "np.logical_and(classified_data_region, mask)\n",
    ") + np.sum(np.logical_and(np.logical_not(classified_data_region), np.logical_not(mask)))\n",
    "total_samples = len(testxx) * len(testyy)\n",
    "\n",
    "# means_arr = means.detach().cpu().numpy()\n",
    "# sigma_arr = sigma.detach().cpu().numpy()\n",
    "mean_mse = np.mean(np.square(means_arr[mask] - gt_mean[mask]))\n",
    "mean_sigma = np.mean(np.square(np.square(sigma_arr[mask]) - gt_var[mask]))\n",
    "print(f\"Mean MSE: {mean_mse}\")\n",
    "print(f\"Mean Sigma: {mean_sigma}\")\n",
    "print(f\"Percentage of samples classified correctly: {samples_correct/total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d8a20e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "4518d872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "2 0\n",
      "4 0\n",
      "6 0\n",
      "8 0\n",
      "0 2\n",
      "2 2\n",
      "4 2\n",
      "6 2\n",
      "8 2\n",
      "0 4\n",
      "2 4\n",
      "4 4\n",
      "6 4\n",
      "8 4\n",
      "0 6\n",
      "2 6\n",
      "4 6\n",
      "6 6\n",
      "8 6\n",
      "0 8\n",
      "2 8\n",
      "4 8\n",
      "6 8\n",
      "8 8\n"
     ]
    }
   ],
   "source": [
    "arr = []\n",
    "grid = np.arange(100).reshape(10,10)\n",
    "testx, testy = np.meshgrid(np.arange(10, step=2), np.arange(10, step=2))\n",
    "for x, y in zip(testx.ravel(), testy.ravel()):\n",
    "    print(x, y)\n",
    "    arr.append(grid[x:x+2, y:y+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "afb49da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1, 10, 11, 20, 21, 30, 31, 40, 41],\n",
       "       [50, 51, 60, 61, 70, 71, 80, 81, 90, 91],\n",
       "       [ 2,  3, 12, 13, 22, 23, 32, 33, 42, 43],\n",
       "       [52, 53, 62, 63, 72, 73, 82, 83, 92, 93],\n",
       "       [ 4,  5, 14, 15, 24, 25, 34, 35, 44, 45],\n",
       "       [54, 55, 64, 65, 74, 75, 84, 85, 94, 95],\n",
       "       [ 6,  7, 16, 17, 26, 27, 36, 37, 46, 47],\n",
       "       [56, 57, 66, 67, 76, 77, 86, 87, 96, 97],\n",
       "       [ 8,  9, 18, 19, 28, 29, 38, 39, 48, 49],\n",
       "       [58, 59, 68, 69, 78, 79, 88, 89, 98, 99]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(arr).reshape((10,10), order=\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "77588461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n",
       "       [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
       "       [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "       [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n",
       "       [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n",
       "       [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],\n",
       "       [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],\n",
       "       [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "np.hstack([np.vstack(arr[i:i+5,:,:]) for i in range(0, 25, 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "76366ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_gaps(gaps, shape):\n",
    "    mask = np.ones(shape)\n",
    "    for gap in gaps:\n",
    "        xx, yy = np.meshgrid(list(range(gap[0], gap[1] + 1)), list(range(gap[2], gap[3] + 1)))\n",
    "        mask[xx, yy] = 0\n",
    "    return mask.flatten()\n",
    "def polyf(x, y):\n",
    "    return x ** 2 + y ** 2\n",
    "\n",
    "def varf(x, y):\n",
    "    return 0.5 * x + 0.5 * y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "93722e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3c0b3a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "09e5c640",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[174], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m samples_correct \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39mlogical_and(classified_data_region, mask)\n\u001b[1;32m      3\u001b[0m ) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mlogical_and(np\u001b[38;5;241m.\u001b[39mlogical_not(classified_data_region), np\u001b[38;5;241m.\u001b[39mlogical_not(mask)))\n\u001b[1;32m      4\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(testx) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(testy)\n\u001b[0;32m----> 6\u001b[0m means \u001b[38;5;241m=\u001b[39m \u001b[43mmeans\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      7\u001b[0m sigma \u001b[38;5;241m=\u001b[39m sigma\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m mean_mse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39msquare(means[mask] \u001b[38;5;241m-\u001b[39m gt_mean[mask]))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'detach'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0e69dde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True, False,\n",
       "       False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True, False, False, False, False,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "       False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True, False, False, False, False,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "39096829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True, False, False, False,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True, False, False, False,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True, False, False, False,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "        False, False,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5131c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hw4dl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhw4dl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtoy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolyData\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhw4dl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanage_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model, get_most_recent_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hw4dl'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def score_network_performance(model:nn.Module,\n",
    "                             test_loader:Map2Loc,\n",
    "                             epi_threshold:float=0.1,\n",
    "                             task = \"patch\",\n",
    "                             device:str=\"cpu\",\n",
    "                             )->plt.Axes:\n",
    "  samples = np.linspace(toy_loader.lower, toy_loader.upper, 1000)\n",
    "  var = toy_loader.varf(samples)\n",
    "  # ax.plot(samples, toy_loader.polyf(samples), label=\"True Function\")\n",
    "\n",
    "  torch_input = torch.unsqueeze(torch.tensor(samples, dtype=torch.float32), 1).to(device)\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  model.scramble_batches = False\n",
    "  outputs = model(torch_input)\n",
    "  values = torch.stack(outputs).squeeze(-1)\n",
    "  means = values[:,:,0].mean(dim=0)\n",
    "  sigma = torch.sqrt(torch.mean(make_sigma_positive(values[:,:,1]) + torch.square(values[:,:,0]), dim=0) - torch.square(means))\n",
    "  epistemic_sigma = torch.std(values[:,:,0], dim=0)\n",
    "  epistemic_sigma = epistemic_sigma.detach().cpu().numpy()\n",
    "\n",
    "  classified_data_region = epistemic_sigma < epi_threshold\n",
    "  intervals = construct_intervals(toy_loader.use_gaps, toy_loader.gaps, toy_loader.lower, toy_loader.upper)\n",
    "  mask = get_mask_from_intervals(intervals, samples)\n",
    "  samples_correct = np.sum(\n",
    "    np.logical_and(classified_data_region, mask)\n",
    "  ) + np.sum(np.logical_and(np.logical_not(classified_data_region), np.logical_not(mask)))\n",
    "  total_samples = samples.shape[0]\n",
    "\n",
    "\n",
    "  means = means.detach().cpu().numpy()\n",
    "  sigma = sigma.detach().cpu().numpy()\n",
    "  mean_mse = np.mean(np.square(means[mask] - toy_loader.polyf(samples[mask])))\n",
    "  mean_sigma = np.mean(np.square(np.square(sigma[mask]) - var[mask]))\n",
    "  print(f\"Mean MSE: {mean_mse}\")\n",
    "  print(f\"Mean Sigma: {mean_sigma}\")\n",
    "  print(f\"Percentage of samples classified correctly: {samples_correct/total_samples}\")\n",
    "  return mean_mse, mean_sigma, samples_correct/total_samples\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: check valid interval points, calculate amount of lower,upper that it classified correctly\n",
    "\n",
    "def get_mask_from_intervals(intervals, x_values:np.ndarray):\n",
    "  mask = np.zeros_like(x_values, dtype=bool)\n",
    "  for interval in intervals:\n",
    "    mask = np.logical_or(mask, np.logical_and(x_values >= interval[0], x_values <= interval[1]))\n",
    "  return mask\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  most_recent_model = get_most_recent_model()\n",
    "  model, config = load_model(most_recent_model)\n",
    "  polyf, varf, gaps = make_polyf(config[\"polyf_type\"])\n",
    "  train_dataset = PolyData(polyf, varf, gaps, size=config[\"train_size\"], seed=1111)\n",
    "\n",
    "  score_network_performance(model, train_dataset, epi_threshold=0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpips2",
   "language": "python",
   "name": "lpips2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
